#----------------------------------------------------------------------
# Fit the full model, where all independent variables are included
full_model  <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature + Pressure, data = Ozone_data)
# Test all possible subsets
output      <- ols_step_all_possible(full_model)
# Print results from all possible subsets
output
# Plot results from all possible subsets
plot(output)
#----------------------------------------------------------------------
ols_step_best_subset(full_model)
ols_step_backward_aic(full_model, details=F)
ols_step_forward_aic(full_model, details=F)
ols_step_both_aic(full_model, details=F)
ols_step_backward_aic(full_model, details=T)
#----------------------------------------------------------------------
# Best model
reg <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature , data = Ozone_data)
summary(reg)
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# Make confidence band
predict(reg, interval="confidence", level=0.95)
# Make prediction band
predict(reg, interval="prediction", level=0.95)
# 1.1 Load data
library(MASS)
data(cpus)
# 1.2 Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
cpus_validation
# 1.3 Plot scatter plots
ggpairs(cpus_training, columns=2:9)
# 1.4 Full model
full_model <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmin + chmax, data = cpus_training)
summary(full_model)
# Variable selection
# 1.5 Use the all possible subsets approach to find the best model
ols_step_all_possible(full_model)
# 1.6 Use the all best subsets approach to find the best model
ols_step_best_subset(full_model)
# 1.7 Use backward elimination approach to find the best model.
ols_step_backward_aic(full_model)
# 1.8 Use forward selection approach to find the best model.
ols_step_forward_aic(full_model)
# 1.9 Use stepwise regression approach to find the best model.
ols_step_both_aic(full_model)
# 1.10 Best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
summary(reg)
# Check assumptions
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# 1.11 Predicted individual CPU performance of the validation subset
pred <- predict(reg, cpus_validation,interval="prediction", level=0.95)
# Compare
plot(pred[,1],cpus_validation$perf, xlab="Predicted CPU performance",
ylab="Actual CPU performance")
# Get Pearson correlation coefficient and relative mean bias
cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias <- mean(pred[,1]) - mean(cpus_validation$perf)
mean_bias/mean(cpus_validation$perf)*100
# 1.12 Simulations
N_simulations     <- 50
correlation_all   <- c()
relative_bias_all <- c()
# Loop each simulation
for(i in 1:N_simulations){
# Create new subsets each time
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Using the same best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
# Mean Pearson correlation coefficient
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
mean(correlation_all) # ~ 0.90, not bad
# 1.12 Simulations
N_simulations     <- 50
correlation_all   <- c()
relative_bias_all <- c()
# Loop each simulation
for(i in 1:N_simulations){
# Create new subsets each time
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Using the same best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
# Mean Pearson correlation coefficient
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
mean(correlation_all) # ~ 0.90, not bad
# 1.12 Simulations
N_simulations     <- 50
correlation_all   <- c()
relative_bias_all <- c()
# Loop each simulation
for(i in 1:N_simulations){
# Create new subsets each time
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Using the same best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
# Mean Pearson correlation coefficient
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
mean(correlation_all) # ~ 0.90, not bad
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all) # 1-2%, very good!
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all) # 1-2%, very good!
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all) # 1-2%, very good!
library(ggplot2)
library(GGally)
library(olsrr)
# Read ozone and meteorological data
Ozone_data <- read.csv(file = "ozone_data.csv", header=T)
# Check the data
str(Ozone_data)
# Plot scatter plots
ggpairs(Ozone_data, columns=1:5)
#----------------------------------------------------------------------
# Fit the full model, where all independent variables are included
full_model  <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature + Pressure, data = Ozone_data)
full_model
summary(full_model)
output      <- ols_step_all_possible(full_model)
# Print results from all possible subsets
output
output      <- ols_step_all_possible(full_model)
# Print results from all possible subsets
output
# Plot results from all possible subsets
plot(output)
# Plot results from all possible subsets
plot(output)
#----------------------------------------------------------------------
ols_step_best_subset(full_model)
ols_step_backward_aic(full_model, details=T)
ols_step_forward_aic(full_model, details=F)
ols_step_forward_aic(full_model, details=T)
ols_step_both_aic(full_model, details=F)
reg <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature , data = Ozone_data)
summary(reg)
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# Make confidence band
predict(reg, interval="confidence", level=0.95)
# Make prediction band
predict(reg, interval="prediction", level=0.95)
q1 <- c(100, 50, 100, 100, 90, 98, 100, 100)
mean(q1)
sd(q1)
q2 <- c(85, 80, 100, 100, 75, 90, 100, 100)
mean(q2)
sd(q2)
q3 <- c(100, 90, 50, 80, 80, 80, 100, 100)
mean(q3)
sd(q3)
q4 <- c(100, 20, 20, 70, 70, 80, 90, 80)
mean(q4)
sd(q4)
install.packages("InformationValue")
install.packages("Rtools")
# Population per km2
Pop <- c(797,  3652,   384,   876,  1156,
5282,  3602,  4305,  6451, 939,
2725,   296,  1187,  4819,  7856,
1074,  1444,  2620,   417,  3232)
# PM exceeding
PM  <- c( 0,     1,    0,   0,   0,
1,     0,    1,   1,   1,
1,     0,    0,   0,   1,
0,     0,    1,   0,   1)
# Make a data frame
PM_data <- data.frame(Pop, PM)
str(PM_data)
# Fit the regression model
logistic <- glm( PM ~ Pop, data = PM_data, family = binomial)
# Print model detail
summary(logistic)
pR2(logistic)["McFadden"]
library(pscl)
pR2(logistic)["McFadden"]
# Find optimal cutoff probability to use to maximize accuracy
predicted_value <- predict(logistic, PM_data, type="response")
optimalCutoff(PM_data$PM, predicted_value)[1]
library(pscl)
library(InformationValue)
# Make a data frame
PM_data <- data.frame(Pop, PM)
str(PM_data)
# Fit the regression model
logistic <- glm( PM ~ Pop, data = PM_data, family = binomial)
# Print model detail
summary(logistic)
pR2(logistic)["McFadden"]
# Find optimal cutoff probability to use to maximize accuracy
predicted_value <- predict(logistic, PM_data, type="response")
optimalCutoff(PM_data$PM, predicted_value)[1]
# Define new cities where we want to make predictions
new_cities <- data.frame(Pop = c(1000, 5000))
#predict probability of defaulting
predict(logistic, new_cities, type="response")
# Add 5 cities to the Section Example
# Population per km2
Pop <- c(797,  3652,   384,   876,  1156,
5282,  3602,  4305,  6451, 939,
2725,   296,  1187,  4819,  7856,
1074,  1444,  2620,   417,  3232,
1988, 4000, 607, 5001, 7890)
# PM exceeding
PM  <- c( 0,     1,    0,   0,   0,
1,     0,    1,   1,   1,
1,     0,    0,   0,   1,
0,     0,    1,   0,   1,
0,     1,    0,   1,   1)
# Make a data frame
PM_data <- data.frame(Pop, PM)
str(PM_data)
# Fit the regression model
logistic <- glm( PM ~ Pop, data = PM_data, family = binomial)
# Print model detail
summary(logistic)
pR2(logistic)["McFadden"]
# Find optimal cutoff probability to use to maximize accuracy
predicted_value <- predict(logistic, PM_data, type="response")
optimalCutoff(PM_data$PM, predicted_value)[1]
# Define new cities where we want to make predictions
new_cities <- data.frame(Pop = c(300, 1500, 30000))
#predict probability of defaulting
predict(logistic, new_cities, type="response")
# Read data
Keeling_Data     <- read.csv(file = "co2_mm_mlo.csv", header = T)
# Handel missing values
Keeling_Data$co2[which(Keeling_Data$co2<0)] <- NA
for(i in 1:length(Keeling_Data$co2)){
if( is.na(Keeling_Data$co2[i])){
Keeling_Data$co2[i] <- mean(Keeling_Data$co2[(i-2):(i+2)],na.rm=T )
}
}
# Apply the ts() function
co2 <- ts(Keeling_Data$co2, start=c(1958,3), frequency=12)
co2
# Quick plot
plot(co2, type="l")
co2_components <- decompose(co2)
plot(co2_components)
# Plot hist
hist(co2_components$random, prob=TRUE)
# Add a normal pdf curve
curve(dnorm(x, mean=mean(co2_components$random, na.rm=T),
sd=sd(co2_components$random, na.rm=T)),
add=TRUE, col="red")
#--------------------------------------------------------------
# Make two panels
par(mfrow=c(2,1))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==+.9)))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==-.9)))
#--------------------------------------------------------------
# Make two panels
par(mfrow=c(2,1))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==+.9)))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==-.9)))
#--------------------------------------------------------------
# Make two panels
par(mfrow=c(2,1))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==+.9)))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==-.9)))
#--------------------------------------------------------------
# Make two panels
par(mfrow=c(2,1))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==+.9)))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==-.9)))
#--------------------------------------------------------------
# Make two panels
par(mfrow=c(2,1))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==+.9)))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==-.9)))
# Make two panels
par(mfrow=c(2,1))
# ARIMA model (0,0,1)
plot(arima.sim(list(order=c(0,0,1), ma=.9), n=100), ylab="x",
main=(expression(MA(1)~~~theta==+.9)))
# ARIMA model (0,0,1)
plot(arima.sim(list(order=c(0,0,1), ma=-.9), n=100), ylab="x",
main=(expression(MA(1)~~~theta==-.9)))
# Compute ACF and PACF of AR(2)
ACF = ARMAacf(ar=c(1.5,-.75), ma=0, 24)[-1]
PACF = ARMAacf(ar=c(1.5,-.75), ma=0, 24, pacf=TRUE)
# Plot ACF and PACF of AR(2)
par(mfrow=c(1,2))
plot(ACF, type="h", xlab="lag", ylim=c(-.8,1)); abline(h=0)
plot(PACF, type="h", xlab="lag", ylim=c(-.8,1)); abline(h=0)
# Compute ACF and PACF of MA(2)
ACF = ARMAacf(ar=0, ma=c(0.5,0.6), 24)[-1]
PACF = ARMAacf(ar=0, ma=c(0.5,0.6), 24, pacf=TRUE)
# Plot ACF and PACF of MA(2)
par(mfrow=c(1,2))
plot(ACF, type="h", xlab="lag", ylim=c(-.8,1)); abline(h=0)
plot(PACF, type="h", xlab="lag", ylim=c(-.8,1)); abline(h=0)
ACF = ARMAacf(ar=c(1.5,-.75), ma=0, 24)[-1]
PACF = ARMAacf(ar=c(1.5,-.75), ma=0, 24, pacf=TRUE)
# Plot ACF and PACF of AR(2)
par(mfrow=c(1,2))
plot(ACF, type="h", xlab="lag", ylim=c(-.8,1)); abline(h=0)
plot(PACF, type="h", xlab="lag", ylim=c(-.8,1)); abline(h=0)
#-------------------------------------------------------------------
# Load data
par(mfrow=c(1,1))
library(astsa)
# Plot
plot(globtemp, type="l", ylab="Global Temperature Deviations")
# Fit a linear model
trModel <- lm(globtemp ~ c(1:length(globtemp)))
trModel
# Get the residuals
residual <- resid(trModel)
# Plot the residual
plot(residual, type="l")
# Check acf anf pacf of the residuals
acf(residual)
pacf(residual)
# EX 1 Load data
par(mfrow=c(1,1))
library(astsa)
# Plot
plot(soi, type="l", ylab="Southern Oscillation Index")
# Fit a linear model
trModel <- lm(soi ~ c(1:length(soi)))
trModel
# Get the residuals
residual <- resid(trModel)
# Plot the residual
plot(residual, type="l")
# Plot
plot(soi, type="l", ylab="Southern Oscillation Index")
soi
soi_components <- decompose(soi)
plot(soi_components)
par(mfrow=c(1,1))
library(astsa)
# Plot
plot(soi, type="l", ylab="Southern Oscillation Index")
soi_components <- decompose(soi)
plot(soi_components)
soi
soi_components
soi_components$seasonal
soi_deseasonalized <- soi-soi_components$seasonal
soi_deseasonalized
# Check acf anf pacf of the residuals
acf(residual)
# Check acf anf pacf of the residuals
acf(soi_deseasonalized)
pacf(soi_deseasonalized)
# Check acf anf pacf of the residuals
acf(soi_deseasonalized)
pacf(soi_deseasonalized)
setwd("C://ese335")
rmarkdown::render_site()
rmarkdown::render_site()
# Read data
Keeling_Data     <- read.csv(file = "co2_mm_mlo.csv", header = T)
# Handel missing values
Keeling_Data$co2[which(Keeling_Data$co2<0)] <- NA
for(i in 1:length(Keeling_Data$co2)){
if( is.na(Keeling_Data$co2[i])){
Keeling_Data$co2[i] <- mean(Keeling_Data$co2[(i-2):(i+2)],na.rm=T )
}
}
# Apply the ts() function
co2 <- ts(Keeling_Data$co2, start=c(1958,3), frequency=12)
# Quick plot
plot(co2, type="l")
co2
# Apply the ts() function
co2 <- ts(Keeling_Data$co2, start=c(1958,3), frequency=12)
# Quick plot
plot(co2, type="l")
co2 <- ts(Keeling_Data$co2, start=c(1958,3), frequency=12)
# Quick plot
plot(co2, type="l")
co2_components <- decompose(co2)
plot(co2_components)
# Plot hist
hist(co2_components$random, prob=TRUE)
hist(co2_components$random, prob=TRUE)
# Add a normal pdf curve
curve(dnorm(x, mean=mean(co2_components$random, na.rm=T),
sd=sd(co2_components$random, na.rm=T)),
add=TRUE, col="red")
# Make two panels
par(mfrow=c(2,1))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==+.9)))
# ARIMA model (1,0,0)
plot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab="x",
main=(expression(AR(1)~~~phi==-.9)))
par(mfrow=c(1,1))
library(astsa)
globtemp
plot(globtemp, type="l", ylab="Global Temperature Deviations")
# Fit a linear model
trModel <- lm(globtemp ~ c(1:length(globtemp)))
trModel
# Get the residuals
residual <- resid(trModel)
# Plot the residual
plot(residual, type="l")
# Check acf anf pacf of the residuals
acf(residual)
pacf(residual)
co2_components <- decompose(co2)
co2_components
co2_components$seasonal
#-------------------------------------------------------------------
# EX 1 Load data
par(mfrow=c(1,1))
library(astsa)
# Plot
plot(soi, type="l", ylab="Southern Oscillation Index")
soi_components <- decompose(soi)
plot(soi_components)
# De-seasonalize the data
soi_deseasonalized <- soi-soi_components$seasonal
# Check acf anf pacf of the residuals
acf(soi_deseasonalized)
pacf(soi_deseasonalized)
# Check acf anf pacf of the deseasonalized soi
acf(soi_deseasonalized)
pacf(soi_deseasonalized)
