summary(res_aov)
# Check homoscedasticity of residuals
# not bad
par(mfrow=c(2,2))
plot(res_aov)
par(mfrow=c(1,1))
# Perform ANOVA
res_aov <- aov(Oxygen_isotopic ~ Bone, data=oxygen_data)
summary(res_aov)
TukeyHSD(res_aov)
#-----------------------------------------------------------
# EX 4
#-----------------------------------------------------------
Preg_nonv <- c(185, 189, 187, 181, 150, 176)
Preg_veg  <- c(171, 174, 202, 171, 207, 125, 189,
179, 163, 174, 184, 186)
Nonp_veg  <- c(210, 139, 172, 198, 177)
# Make data frame
zinc_data <- data.frame(Zinc_content = c(Preg_nonv, Preg_veg, Nonp_veg),
Group  = c(rep("Pregnant nonvegetarians",length(Preg_nonv)),
rep("Pregnant vegetarians",length(Preg_veg)),
rep("Nonpregnant vegetarians",length(Nonp_veg)) ))
# Quick plots
ggplot(zinc_data, aes(x = Group, y = Zinc_content, fill = Group)) +
geom_boxplot() +
theme_classic()
# Check independence of the dependent variable
runs.test(zinc_data$Zinc_content)
# Check normality
shapiro.test(zinc_data$Zinc_content)
# Check outliers
# treat 125 not as an outlier
# In fact, removing this one does not change the statistical significance
rosnerTest(zinc_data$Zinc_content, k=2)
grubbs.test(zinc_data$Zinc_content)
# One-way ANOVA
oneway.test(Zinc_content ~ Group, data=zinc_data, var.equal=T)
res_aov <- aov(Zinc_content ~ Group, data=zinc_data)
summary(res_aov)
# Check homoscedasticity of residuals
# not bad
par(mfrow=c(2,2))
plot(res_aov)
par(mfrow=c(1,1))
#-----------------------------------------------------------
# EX 1
#-----------------------------------------------------------
# including a graphical display, a conclusion about the degree of evidence of a difference,
# and a conclusion about # the size of the difference in distributions
Survived <- c(0.687, 0.703, 0.709, 0.715, 0.721, 0.723, 0.723, 0.726, 0.728, 0.728, 0.728, 0.729, 0.730, 0.730,
0.733, 0.733, 0.735, 0.736, 0.739, 0.741, 0.741, 0.741, 0.741, 0.743, 0.749, 0.751, 0.752, 0.752,
0.755, 0.756, 0.766, 0.767, 0.769, 0.770, 0.780)
Perished <- c(0.659, 0.689, 0.702, 0.703, 0.709, 0.713, 0.720, 0.720, 0.726, 0.726, 0.729, 0.731, 0.736, 0.737,
0.738, 0.738, 0.739, 0.743, 0.744, 0.745, 0.752, 0.752, 0.754, 0.765)
# Make data frame
Humerus_data <- data.frame(Lengths = c(Survived, Perished),
Status  = c(rep("Survived",length(Survived)),rep("Perished",length(Perished))))
# Compare boxplots
Humerus_data %>%
ggplot(aes(x=as.character(Status), y=Lengths)) +
geom_boxplot(fill="steelblue") +
labs(title="Humerus lengths by survival status", x="Survival status", y="Length (inch)")
# Check normality
lillie.test(Survived)
lillie.test(Perished)
#-----------------------------------------------------------
# EX 2
#-----------------------------------------------------------
# Brain Size and Litter Size
Small_litter <- c(0.42, 0.86, 0.88, 1.11, 1.34, 1.38, 1.42, 1.47, 1.63, 1.73, 2.17, 2.42,
2.48, 2.74, 2.74, 2.79, 2.90, 3.12, 3.18, 3.27, 3.30, 3.61, 3.63, 4.13,
4.40, 5.00, 5.20, 5.59, 7.04, 7.15, 7.25, 7.75, 8.00, 8.84, 9.30, 9.68,
10.32, 10.41, 10.48, 11.29, 12.30, 12.53, 12.69, 14.14, 14.15, 14.27,
14.56, 15.84, 18.55, 19.73, 20.00)
Large_litter <- c(0.94, 1.26, 1.44, 1.49, 1.63, 1.80, 2.00, 2.00, 2.56, 2.58, 3.24, 3.39,
3.53, 3.77, 4.36, 4.41, 4.60, 4.67, 5.39, 6.25, 7.02, 7.89, 7.97, 8.00,
8.28, 8.83, 8.91, 8.96, 9.92, 11.36, 12.15, 14.40, 16.00, 18.61, 18.75, 19.05,
21.00, 21.41, 23.27, 24.71, 25.00, 28.75, 30.23, 35.45, 36.35)
# Check normality
lillie.test(Small_litter)
lillie.test(Large_litter)
# Check outliers
# No outlier
rosnerTest(Small_litter, k=2)
rosnerTest(Large_litter, k=2)
grubbs.test(Small_litter)
grubbs.test(Large_litter)
# Large sample size, use t-test
t.test(Small_litter, Large_litter, alternative = "two.sided", paired = F, var.equal = F)
library(ggplot2)
library(randtests)
library(moments)
library(outliers)
library(EnvStats)
library(nortest)
library(ggpubr)
library(BSDA)
library(FSA)
#-----------------------------------------------------------
# EX 1
#-----------------------------------------------------------
# including a graphical display, a conclusion about the degree of evidence of a difference,
# and a conclusion about # the size of the difference in distributions
Survived <- c(0.687, 0.703, 0.709, 0.715, 0.721, 0.723, 0.723, 0.726, 0.728, 0.728, 0.728, 0.729, 0.730, 0.730,
0.733, 0.733, 0.735, 0.736, 0.739, 0.741, 0.741, 0.741, 0.741, 0.743, 0.749, 0.751, 0.752, 0.752,
0.755, 0.756, 0.766, 0.767, 0.769, 0.770, 0.780)
Perished <- c(0.659, 0.689, 0.702, 0.703, 0.709, 0.713, 0.720, 0.720, 0.726, 0.726, 0.729, 0.731, 0.736, 0.737,
0.738, 0.738, 0.739, 0.743, 0.744, 0.745, 0.752, 0.752, 0.754, 0.765)
# Make data frame
Humerus_data <- data.frame(Lengths = c(Survived, Perished),
Status  = c(rep("Survived",length(Survived)),rep("Perished",length(Perished))))
# Compare boxplots
Humerus_data %>%
ggplot(aes(x=as.character(Status), y=Lengths)) +
geom_boxplot(fill="steelblue") +
labs(title="Humerus lengths by survival status", x="Survival status", y="Length (inch)")
# Check normality
lillie.test(Survived)
lillie.test(Perished)
# Perform Runs test
runs.test(Survived)
runs.test(Perished)
# Check outliers
rosnerTest(Survived, k=2)
rosnerTest(Perished, k=2)
grubbs.test(Survived)
grubbs.test(Perished)
# Seems the lowest (0.659) in Perished group is an outlier
# Follow DISPLAY 3.6 Examination strategy
# with the Outlier (0.659)
t.test(Survived, Perished, alternative = "two.sided", paired = F, var.equal = F)
# without the Outlier (0.659)
Perished_new <- Perished[-1]
rosnerTest(Perished_new, k=2) # no more outlier
t.test(Survived, Perished_new, alternative = "two.sided", paired = F, var.equal = F)
# The outlier does not change the output (no significant difference)
# so we INCLUDE the outlier in the report
#-----------------------------------------------------------
# EX 2
#-----------------------------------------------------------
# Brain Size and Litter Size
Small_litter <- c(0.42, 0.86, 0.88, 1.11, 1.34, 1.38, 1.42, 1.47, 1.63, 1.73, 2.17, 2.42,
2.48, 2.74, 2.74, 2.79, 2.90, 3.12, 3.18, 3.27, 3.30, 3.61, 3.63, 4.13,
4.40, 5.00, 5.20, 5.59, 7.04, 7.15, 7.25, 7.75, 8.00, 8.84, 9.30, 9.68,
10.32, 10.41, 10.48, 11.29, 12.30, 12.53, 12.69, 14.14, 14.15, 14.27,
14.56, 15.84, 18.55, 19.73, 20.00)
Large_litter <- c(0.94, 1.26, 1.44, 1.49, 1.63, 1.80, 2.00, 2.00, 2.56, 2.58, 3.24, 3.39,
3.53, 3.77, 4.36, 4.41, 4.60, 4.67, 5.39, 6.25, 7.02, 7.89, 7.97, 8.00,
8.28, 8.83, 8.91, 8.96, 9.92, 11.36, 12.15, 14.40, 16.00, 18.61, 18.75, 19.05,
21.00, 21.41, 23.27, 24.71, 25.00, 28.75, 30.23, 35.45, 36.35)
# Check normality
lillie.test(Small_litter)
lillie.test(Large_litter)
# Check outliers
# No outlier
rosnerTest(Small_litter, k=2)
rosnerTest(Large_litter, k=2)
grubbs.test(Small_litter)
grubbs.test(Large_litter)
# Large sample size, use t-test
t.test(Small_litter, Large_litter, alternative = "two.sided", paired = F, var.equal = F)
#-----------------------------------------------------------
# EX 3
#-----------------------------------------------------------
Rib_16           <- c(11.10, 11.22, 11.29, 11.49)
Gastralia1       <- c(11.32, 11.40, 11.71)
Gastralia2       <- c(11.60, 11.78, 12.05)
Dorsal_vertebra1 <- c(10.61, 10.88, 11.12, 11.24, 11.43)
Dorsal_vertebra2 <- c(10.92, 11.20, 11.30, 11.62, 11.70)
Femur            <- c(11.70, 11.79, 11.91, 12.15)
Tibia            <- c(11.33, 11.41, 11.62, 12.15, 12.30)
Metatarsal       <- c(11.32, 11.65, 11.96, 12.15)
Phalange         <- c(11.54, 11.89, 12.04)
Proximal_caudal  <- c(10.93, 11.01, 11.08, 11.12, 11.28, 11.37)
Mid_caudal       <- c(11.35, 11.43, 11.50, 11.57, 11.92)
Distal_caudal    <- c(11.95, 12.01, 12.25, 12.30, 12.39)
# Make data frame
oxygen_data <- data.frame( Oxygen_isotopic = c(Rib_16, Gastralia1, Gastralia2, Dorsal_vertebra1,
Dorsal_vertebra2, Femur, Tibia, Metatarsal, Phalange,
Proximal_caudal, Mid_caudal, Distal_caudal),
Bone  = c(rep("Rib_16",length(Rib_16))        , rep("Gastralia1",length(Gastralia1))            ,
rep("Gastralia2",length(Gastralia2)), rep("Dorsal_vertebra1",length(Dorsal_vertebra1)),
rep("Dorsal_vertebra2",length(Dorsal_vertebra2)), rep("Femur",length(Femur))          ,
rep("Tibia",length(Tibia))          , rep("Metatarsal",length(Metatarsal))            ,
rep("Phalange",length(Phalange))    , rep("Proximal_caudal",length(Proximal_caudal))  ,
rep("Mid_caudal",length(Mid_caudal)), rep("Distal_caudal",length(Distal_caudal)) ))
# Boxplots
ggplot(oxygen_data, aes(x=Bone, y=Oxygen_isotopic, group=Bone, fill=Bone)) +
geom_boxplot() +
labs(title="Oxygen isotopic composition of vertebrate bone phosphate",
x="Bone", y="Oxygen isotopic composition [per mil deviations from SMOW]") +
theme_classic()
# Perform the Kruskalâ€“Wallis test
kruskal.test(Oxygen_isotopic ~ Bone, data = oxygen_data)
# Post-hoc test
dunnTest(Oxygen_isotopic ~ Bone, data=oxygen_data, method="bh")
# Perform ANOVA
res_aov <- aov(Oxygen_isotopic ~ Bone, data=oxygen_data)
summary(res_aov)
library(MASS)
data(cpus)
fraction     <- 0.85
sample_index <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_train   <- cpus[sample_index,]
cpus_test    <- cpus[-sample_index,]
library(MASS)
data(cpus)
# Partitioning training (85%) and validation (15%) subsets
fraction     <- 0.85
sample_index <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_train   <- cpus[sample_index,]
cpus_test    <- cpus[-sample_index,]
cpus
# Plot scatter plots
ggpairs(cpus, columns=2:9)
library(ggplot2)
library(GGally)
library(olsrr)
# Plot scatter plots
ggpairs(cpus, columns=2:9)
# Load data
library(MASS)
data(cpus)
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Plot scatter plots
ggpairs(cpus_training, columns=2:9)
# Full model
model <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmin + chmax, data = cpus_training)
model
full_model
# Full model
full_model <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmin + chmax, data = cpus_training)
full_model
summary(full_model)
# Use the all possible subsets approach to find the best model
ols_step_all_possible(full_model)
ols_step_best_subset(full_model)
# 1.10 Best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_train)
summary(reg)
# Predicted individual CPU performance of the validation subset
pred <- predict(reg, cpus_validation,interval="prediction", level=0.95)
pred
# Compare
plot(pred[,1],cpus_validation$perf)
# Compare
plot(pred[,1],cpus_validation$perf, xlab="Predicted CPU performance",
ylab="Actual CPU performance")
# Check assumptions
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# Get Pearson correlation coefficient and relative mean bias
correlation <- cor(pred[,1],cpus_validation$perf)
# Get Pearson correlation coefficient and relative mean bias
cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
Mean_bias <- mean(pred[,1]) - mean(cpus_validation$perf)
Mean_bias/mean(cpus_validation$perf)*100
# Best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
pred
# 1.12 Simulations
N_simulations     <- 50
correlation_all   <- c()
relative_bias_all <- c()
for(i in 1:N_simulations){
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
correlation_all
relative_bias_all
hist(correlation_all)
hist(relative_bias_all)
vline(mean(correlation_all), col="red")
abline(v=mean(correlation_all), col="red")
abline(v=mean(correlation_all), col="red", lwd=2, type=2)
abline(v=mean(correlation_all), col="red", lwd=2, lty=2)
hist(correlation_all)
abline(v=mean(correlation_all), col="red", lwd=2)
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all)
mean(relative_bias_all)
mean(correlation_all)
mean(relative_bias_all) #
library(ggplot2)
library(GGally)
library(olsrr)
# Read ozone and meteorological data
Ozone_data <- read.csv(file = "ozone_data.csv", header=T)
# Check the data
str(Ozone_data)
# Plot scatter plots
ggpairs(Ozone_data, columns=1:5)
#----------------------------------------------------------------------
# Fit the full model, where all independent variables are included
full_model  <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature + Pressure, data = Ozone_data)
# Test all possible subsets
output      <- ols_step_all_possible(full_model)
# Print results from all possible subsets
output
# Plot results from all possible subsets
plot(output)
#----------------------------------------------------------------------
ols_step_best_subset(full_model)
ols_step_backward_aic(full_model, details=F)
ols_step_forward_aic(full_model, details=F)
ols_step_both_aic(full_model, details=F)
#----------------------------------------------------------------------
# Best model
reg <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature , data = Ozone_data)
summary(reg)
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# Make confidence band
predict(reg, interval="confidence", level=0.95)
# Make prediction band
predict(reg, interval="prediction", level=0.95)
#----------------------------------------------------------------------
# EX 1
# 1.1 Load data
library(MASS)
data(cpus)
# 1.2 Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# 1.3 Plot scatter plots
ggpairs(cpus_training, columns=2:9)
# 1.4 Full model
full_model <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmin + chmax, data = cpus_training)
summary(full_model)
# Variable selection
# 1.5 Use the all possible subsets approach to find the best model
ols_step_all_possible(full_model)
# 1.6 Use the all best subsets approach to find the best model
ols_step_best_subset(full_model)
# 1.7 Use backward elimination approach to find the best model.
ols_step_backward_aic(full_model)
# 1.8 Use forward selection approach to find the best model.
ols_step_forward_aic(full_model)
# 1.9 Use stepwise regression approach to find the best model.
ols_step_both_aic(full_model)
# 1.10 Best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
summary(reg)
# Check assumptions
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# 1.11 Predicted individual CPU performance of the validation subset
pred <- predict(reg, cpus_validation,interval="prediction", level=0.95)
# Compare
plot(pred[,1],cpus_validation$perf, xlab="Predicted CPU performance",
ylab="Actual CPU performance")
# Get Pearson correlation coefficient and relative mean bias
cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias <- mean(pred[,1]) - mean(cpus_validation$perf)
mean_bias/mean(cpus_validation$perf)*100
# 1.12 Simulations
N_simulations     <- 50
correlation_all   <- c()
relative_bias_all <- c()
# Loop each simulation
for(i in 1:N_simulations){
# Create new subsets each time
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Using the same best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
# Mean Pearson correlation coefficient
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
mean(correlation_all) # ~ 0.90, not bad
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all) # ~ 0.5%, very good!
# 1.12 Simulations
N_simulations     <- 100000
correlation_all   <- c()
relative_bias_all <- c()
# Loop each simulation
for(i in 1:N_simulations){
# Create new subsets each time
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Using the same best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
# Mean Pearson correlation coefficient
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
mean(correlation_all) # ~ 0.90, not bad
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all) # ~ 1-2%, very good!
# 1.12 Simulations
N_simulations     <- 10000
correlation_all   <- c()
relative_bias_all <- c()
# Loop each simulation
for(i in 1:N_simulations){
# Create new subsets each time
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Using the same best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
# Mean Pearson correlation coefficient
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
mean(correlation_all) # ~ 0.90, not bad
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all) # ~ 1-2%, very good!
setwd("C://ese335")
rmarkdown::render_site()
rmarkdown::render_site()
library(InformationValue)
library(pscl)
# Population per km2
Pop <- c(797,  3652,   384,   876,  1156,
5282,  3602,  4305,  6451, 939,
2725,   296,  1187,  4819,  7856,
1074,  1444,  2620,   417,  3232)
# PM exceeding
PM  <- c( 0,     1,    0,   0,   0,
1,     0,    1,   1,   1,
1,     0,    0,   0,   1,
0,     0,    1,   0,   1)
# Make a data frame
PM_data <- data.frame(Pop, PM)
str(PM_data)
# Fit the regression model
logistic <- glm( PM ~ Pop, data = PM_data, family = binomial)
# Print model detail
summary(logistic)
# Get McFaddenâ€™s R2
pR2(logistic)["McFadden"]
# Find optimal cutoff probability to use to maximize accuracy
predicted_value <- predict(logistic, PM_data, type="response")
optimalCutoff(PM_data$PM, predicted_value)[1]
# Define new cities where we want to make predictions
new_cities <- data.frame(Pop = c(1000, 5000))
#predict probability of defaulting
predict(logistic, new_cities, type="response")
