par(mfrow = c(1, 1))
# Convert to km
Elevation <- c(180, 305, 381, 488, 549, 640, 762, 883)*0.001
Temperature_C <- c(13.3, 12.2, 13.3, 10.0, 8.3, 9.4, 8.3, 7.2)
# Scatter plot
# Do you see a linear trend
plot(Elevation, Temperature_C)
# Check potential outliers
boxplot(Temperature_C)
grubbs.test(Temperature_C)
# Build the model
reg             <- lm(Temperature_C ~ Elevation)
summary(reg)
Surface_density <- c(8.7, 8.5, 9.1, 9.3, 6.3, 7.8, 9.9,
6.5, 6.6, 9.5, 9.4, 7.2, 8.6, 9.6, 11.1)
Pb_content <- c(6.0, 5.7, 6.0, 5.7, 4.7, 5.5, 6.4, 4.2,
6.2, 5.9, 5.7, 5.7, 6.1, 6.3, 6.0)
# Check normality
shapiro.test(Surface_density)
shapiro.test(Pb_content)
# Surface density is NOT normal,
# so we use non-parametric alternative of Pearson test
# The sample size is NOT small and there are NOT may ties,
# so we will use Spearman correlation test.
# Perform the Spearman correlation test
cor.test(Surface_density, Pb_content, method="spearman",
alternative="two.sided", conf.level=0.95)
# Perform the Spearman correlation test
cor.test(Surface_density, Pb_content, method="kendall",
alternative="two.sided", conf.level=0.95)
Pressure_Before <- c(134, 112, 113, 113, 111, 153, 143,
105, 142, 157, 119, 111)
Pressure_After <- c(132, 126, 149, 124, 118, 157, 142,
124, 139, 157, 135, 125)
# Check normality
shapiro.test(Pressure_Before)
shapiro.test(Pressure_After)
# Check outliers
rosnerTest(Pressure_Before, k=2)
rosnerTest(Pressure_After, k=2)
grubbs.test(Pressure_Before)
grubbs.test(Pressure_After)
# Seems NO outliers
# Pressure_After is NOT normal,
# so we use non-parametric alternative of pair-sample t-test.
# Check if the data is symmetrical by looking at skewness
skewness(Pressure_Before)
skewness(Pressure_After)
# Seems approximately symmetrical,
# so we will use Wilcoxon signed rank test
# Perform the Wilcoxon signed rank test,
# Since H0 states about "increase", so we use one-sided p-value
wilcox.test(Pressure_Before, Pressure_After, paired=T, alternative="less")
#-------------------------------------------------------
# Ex. 5 Atmospheric lapse rate
#-------------------------------------------------------
# Convert to km
Elevation <- c(180, 305, 381, 488, 549, 640, 762, 883)*0.001
Temperature_C <- c(13.3, 12.2, 13.3, 10.0, 8.3, 9.4, 8.3, 7.2)
# Scatter plot
# Do you see a linear trend
plot(Elevation, Temperature_C)
# Check potential outliers
boxplot(Temperature_C)
grubbs.test(Temperature_C)
# Build the model
reg             <- lm(Temperature_C ~ Elevation)
summary(reg)
# Sample data
Elevation <- c(180, 305, 381, 488, 549, 640, 762, 883)
Temperature <- c(13.3, 12.2, 13.3, 10.0, 8.3, 9.4, 8.3, 7.2)
# Scatter plot
plot(Elevation,Temperature)
# Perform linear regression
reg <- lm(Elevation ~ Temperature)
summary(reg)
#-------------------------------------------------------
# Ex. 2 Aerosol surface density and Pb content
#-------------------------------------------------------
Surface_density <- c(8.7, 8.5, 9.1, 9.3, 6.3, 7.8, 9.9,
6.5, 6.6, 9.5, 9.4, 7.2, 8.6, 9.6, 11.1)
Pb_content <- c(6.0, 5.7, 6.0, 5.7, 4.7, 5.5, 6.4, 4.2,
6.2, 5.9, 5.7, 5.7, 6.1, 6.3, 6.0)
# Check normality
shapiro.test(Surface_density)
shapiro.test(Pb_content)
# Surface density is NOT normal,
# so we use non-parametric alternative of Pearson test
# The sample size is NOT small and there are some ties,
# so we will use Kendall correlation test.
# Perform the Spearman correlation test
cor.test(Surface_density, Pb_content, method="kendall",
alternative="two.sided", conf.level=0.95)
before<-c(134, 112, 113, 113, 111, 153, 143, 105, 142, 157, 119, 111)
after<-c(132, 126, 149, 124, 118, 157, 142, 124, 139, 157, 135, 125)
runs.test(before)
runs.test(after)
runs.test(before-after)
shapiro.test(before)
shapiro.test(after)
shapiro.test(before-after)
plot(before-after)
dixon.test(before-after)
dixon.test(before-after,opposite = T)
SIGN.test(before, after, alternative = "greater", conf.level = 0.95)
#--> Perform the Wilcoxon signed rank test  <--#
# Since H0 states about "increase", so we use one-sided p-value
wilcox.test(before, after, paired=T, alternative="less")
summary(reg)
library(randtests)
library(outliers)
library(BSDA)
library(ggplot2)
library(FSA)
# 1.Is the dice fair?
N = 300
observed <- c(40, 70, 48, 60, 52, 30)
expected <- rep(N/6, 6)
chisq.test(cbind(observed, expected))
# Data
observed_value <- Die_Rolls
# Expected values
expected_value <- Control_Rolls
# Degrees of freedom
df <- length(expected_value) - 1
# Compute Chi Square statistic
Chi_Square_Statistic <- sum( (observed_value - expected_value)^2/expected_value )
# Compute p-value
p_value <- 1 - pchisq(Chi_Square_Statistic, df, lower.tail = TRUE)
print(p_value)
# 1.Is the dice fair?
N = 300
observed <- c(40, 70, 48, 60, 52, 30)
expected <- rep(N/6, 6)
chisq.test(cbind(observed, expected))
#-------------------------------------------------------
# Ex. 1 Is the dice fair?
#-------------------------------------------------------
# Note:
# you need to use the Chi-square test.
# We didn't cover Chi-square test in the class,
# please check https://www.statisticshowto.com/probability-and-statistics/chi-square/ for more.
# Data
observed_value <- c(40, 70, 48, 60, 52, 30)
# Expected values
expected_value <- c(50, 50, 50, 50, 50, 50)
# Degrees of freedom
df <- length(expected_value) - 1
# Compute Chi Square statistic
Chi_Square_Statistic <- sum( (observed_value - expected_value)^2/expected_value )
# Compute p-value
p_value <- 1 - pchisq(Chi_Square_Statistic, df, lower.tail = TRUE)
print(p_value)
expected
observed
chisq.test(observed, expected)
# 1.Is the dice fair?
N = 300
observed <- c(40, 70, 48, 60, 52, 30)
expected <- rep(N/6, 6)
chisq.test(cbind(observed, expected))
#==============================================================================================#
#==============================================================================================#
#==============================================================================================#
# Note:
# you need to use the Chi-square test.
# We didn't cover Chi-square test in the class,
# please check https://www.statisticshowto.com/probability-and-statistics/chi-square/ for more.
# Data
observed_value <- observed
# Expected values
expected_value <- expected
# Degrees of freedom
df <- length(expected_value) - 1
# Compute Chi Square statistic
Chi_Square_Statistic <- sum( (observed_value - expected_value)^2/expected_value )
# Compute p-value
p_value <- 1 - pchisq(Chi_Square_Statistic, df, lower.tail = TRUE)
print(p_value)
#-------------------------------------------------------
# Ex. 2 Aerosol surface density and Pb content
#-------------------------------------------------------
Surface_density <- c(8.7, 8.5, 9.1, 9.3, 6.3, 7.8, 9.9,
6.5, 6.6, 9.5, 9.4, 7.2, 8.6, 9.6, 11.1)
Pb_content <- c(6.0, 5.7, 6.0, 5.7, 4.7, 5.5, 6.4, 4.2,
6.2, 5.9, 5.7, 5.7, 6.1, 6.3, 6.0)
# Check normality
shapiro.test(Surface_density)
shapiro.test(Pb_content)
# Surface density is NOT normal,
# so we use non-parametric alternative of Pearson test
# The sample size is NOT small and there are some ties,
# so we will use Kendall correlation test.
# Perform the Spearman correlation test
cor.test(Surface_density, Pb_content, method="kendall",
alternative="two.sided", conf.level=0.95)
# import data
Before <- c( 134, 112, 113, 113, 111, 153, 143, 105, 142, 157, 119, 111)
After <- c( 132, 126, 149, 124, 118, 157, 142, 124, 139, 157, 135, 125)
# Check data
# Check independence
runs.test(Before)
runs.test(After)
# Check normality
# Sample size < 30, use the Shapiro-Wilk test
shapiro.test(Before)
shapiro.test(After)
# As Befort is not a normal distribution, so I will use the NP method to
# test if the blood pressure increase after noise
# Perform the Wilcoxon signed rank test
SIGN.test(After, Before, paired=T, alternative="greater")
# As Befort is not a normal distribution, so I will use the NP method to
# test if the blood pressure increase after noise
# Perform the Wilcoxon signed rank test
wilcox.test(After, Before, paired=T, alternative="greater")
# Check if the data is symmetrical by looking at skewness
skewness(Before)
skewness(After)
# Seems approximately symmetrical,
# so we will use Wilcoxon signed rank test
# Perform the Wilcoxon signed rank test,
# Since H0 states about "increase", so we use one-sided p-value
wilcox.test(Before, After, paired=T, alternative="less")
#==============================================================================================#
#==============================================================================================#
#==============================================================================================#
# Check if the data is symmetrical by looking at skewness
skewness(Before)
skewness(After)
hist(Before)
library(BSDA)
library(ggplot2)
library(FSA)
Before<-c(134, 112, 113, 113, 111, 153, 143, 105, 142, 157, 119, 111)
After<-c(132, 126, 149, 124, 118, 157, 142, 124, 139, 157, 135, 125)
shapiro.test(Before)
shapiro.test(After)
SIGN.test(Before, After, alternative = "two.sided", conf.level = 0.95)
#==============================================================================================#
#==============================================================================================#
#==============================================================================================#
Pressure_Before <- Before
Pressure_After  <- After
# Check normality
shapiro.test(Pressure_Before)
shapiro.test(Pressure_After)
# Check outliers
rosnerTest(Pressure_Before, k=2)
rosnerTest(Pressure_After, k=2)
grubbs.test(Pressure_Before)
grubbs.test(Pressure_After)
# Seems NO outliers
# Pressure_After is NOT normal,
# so we use non-parametric alternative of pair-sample t-test.
# Check if the data is symmetrical by looking at skewness
skewness(Pressure_Before)
skewness(Pressure_After)
# Seems approximately symmetrical,
# so we will use Wilcoxon signed rank test
# Perform the Wilcoxon signed rank test,
# Since H0 states about "increase", so we use one-sided p-value
wilcox.test(Pressure_Before, Pressure_After, paired=T, alternative="less")
#==============================================================================================#
#==============================================================================================#
#==============================================================================================#
# Note:
# you need to use the Chi-square test.
# We didn't cover Chi-square test in the class,
# please check https://www.statisticshowto.com/probability-and-statistics/chi-square/ for more.
# Data
observed_value <- dice
# Expected values
expected_value <- expected
# Degrees of freedom
df <- length(expected_value) - 1
# Compute Chi Square statistic
Chi_Square_Statistic <- sum( (observed_value - expected_value)^2/expected_value )
# Compute p-value
p_value <- 1 - pchisq(Chi_Square_Statistic, df, lower.tail = TRUE)
print(p_value)
#Aerosol surface density and Pb content---------------------------
sur_density <- c(8.7, 8.5, 9.1, 9.3, 6.3, 7.8, 9.9, 6.5, 6.6, 9.5, 9.4, 7.2, 8.6, 9.6, 11.1)
pb_content <- c(6.0, 5.7, 6.0, 5.7, 4.7, 5.5, 6.4, 4.2, 6.2, 5.9, 5.7, 5.7, 6.1, 6.3, 6.0)
shapiro.test(sur_density)
shapiro.test(pb_content) #p-value = 0.00682
dixon.test(sur_density)
dixon.test(pb_content)  #p-value = 0.009032
runs.test(sur_density)
runs.test(pb_content)
# Pb content does not follow normal distribution, and there is an outliers in Pb content with many ties
# so I choose Spearman correlation test
plot(sur_density,pb_content)
cor.test(sur_density, pb_content, method="kendall", alternative="two.sided", conf.level=0.95)
#p-value = 0.02703, reject H0
#Noise and blood pressure -----------------------------------------------------
before <- c(134, 112, 113, 113, 111, 153, 143, 105, 142, 157, 119, 111)
after <- c(132, 126, 149, 124, 118, 157, 142, 124, 139, 157, 135, 125)
blood_pres <- data.frame(pressure=c(before,after),
groups=c(rep("before",length(before)),rep("after",length(after))))
blood_pres %>%
ggplot(aes(x=groups,y=pressure,fill=groups)) +
geom_boxplot() +
labs(title="Blood pressure comparison", x="Groups",y="Blood pressure")
ggdensity(before,xlab = "Blood Pressure before Noise Exposure") #not symmetrical
ggdensity(after, xlab = "Blood Pressure after Noise Exposure")
shapiro.test(before) #p-value = 0.04354
shapiro.test(after)
dixon.test(before)
dixon.test(after)
runs.test(before)
runs.test(after)
# The data of before noise is neither normal nor symmetrical, so we should quit paired sample t-test
# and choose Sign test.
#---> This should be "less" <--#
SIGN.test(before,after, alternative = "two.sided", conf.level = 0.95)
#---> This should be "less" <--#
#---> This should be "less" <--#
SIGN.test(before,after, alternative = "less", conf.level = 0.95)
#---> This should be "less" <--#
# Section 19
# COVID-19 daily cases
# Load libraries
library(dplyr)
library(lubridate)
library(forecast)
#-------------------------------------------------------------------------------
# 1. Load the daily new cases data
#-------------------------------------------------------------------------------
# Read in the COVID-19 data
COVID_data <- read.csv(file = "COVID_2020_data.csv", header = T)
# Check the variable names
head(COVID_data)
# Convert the data.frame to a tibble
COVID_tbl <- as_tibble(COVID_data)
setwd("D://ese335")
# Section 19
# COVID-19 daily cases
# Load libraries
library(dplyr)
library(lubridate)
library(forecast)
#-------------------------------------------------------------------------------
# 1. Load the daily new cases data
#-------------------------------------------------------------------------------
# Read in the COVID-19 data
COVID_data <- read.csv(file = "COVID_2020_data.csv", header = T)
# Check the variable names
head(COVID_data)
# Convert the data.frame to a tibble
COVID_tbl <- as_tibble(COVID_data)
# Check the variable names
head(COVID_data)
# Convert the data.frame to a tibble
COVID_tbl <- as_tibble(COVID_data)
COVID_tbl
#-------------------------------------------------------------------------------
# 2. Get the daily total number of newly reported cases worldwide
#-------------------------------------------------------------------------------
# Get global daily new cases
COVID_tbl <- COVID_tbl %>%
mutate(dateRep = as.Date(dateRep,format='%d/%m/%Y')) %>%
group_by(dateRep) %>%
summarize(global_cases = sum(cases))
# Show data
COVID_tbl
# Get global daily new cases
COVID_tbl <- COVID_tbl %>%
mutate(dateRep = as.Date(dateRep,format='%d/%m/%Y')) %>%
group_by(dateRep) %>%
summarize(global_cases = sum(cases))
# Show data
COVID_tbl
# Quick plot
plot(COVID_tbl$dateRep, COVID_tbl$global_cases,
type="l", xlab="Date", ylab="Global cases")
# Quick plot
plot(COVID_tbl$dateRep, COVID_tbl$global_cases,
type="l", xlab="Date", ylab="Global cases")
# Quick plot
plot(COVID_tbl$dateRep, COVID_tbl$global_cases,
type="l", xlab="Date", ylab="Global cases")
#-------------------------------------------------------------------------------
# 4. Filter the data
#-------------------------------------------------------------------------------
# Filter the data, only use data from April 01
COVID_tbl <- COVID_tbl %>%
filter(dateRep >= as.Date("2020-04-01"))
# Show data
COVID_tbl
# Show data
COVID_tbl
# Start date of the time series, read from the .csv file
Date_start <- as.Date("2020-04-01")
# End date of the time series, read from the .csv file
Date_end   <- as.Date("2020-11-09")
Date_start
str(Date_start)
# Get the Julian Day of the start and end date
JD_start   <- yday(Date_start)
JD_end     <- yday(Date_end)
N_days     <- JD_end - JD_start + 1
JD_start
JD_end
N_days
# Convert the vector data to a time series
global_cases_ts <- ts(COVID_tbl$global_cases[1:N_days], start=c(2020,JD_start), frequency=365)
# The indicator of the time series
inds            <- seq(Date_start, Date_end, by="day")
global_cases_ts
inds
# Check structure
str(global_cases_ts)
# Plot time series
plot(inds, global_cases_ts, type="l")
# Plot time series
plot(inds, global_cases_ts, type="l")
# Plot time series
plot(inds, global_cases_ts, type="l")
# Plot time series
plot(inds, global_cases_ts, type="l")
# Data transform with log
global_cases_ts_log <- log(global_cases_ts)
# Plot time series
plot(inds, global_cases_ts_log, type="l")
# Check acf and pacf
acf(global_cases_ts_log)
pacf(global_cases_ts_log)
# Take the diff, d=1
global_cases_ts_log_d1 <- diff(global_cases_ts_log)
# Plot time series
plot(global_cases_ts_log_d1,type="l")
# Check acf and pacf
acf(global_cases_ts_log_d1)
# Check acf and pacf
acf(global_cases_ts_log_d1)
pacf(global_cases_ts_log_d1)
# Check acf and pacf
acf(global_cases_ts_log_d1)
pacf(global_cases_ts_log_d1)
# Automated forecasting using an ARIMA model
best_model <- auto.arima(global_cases_ts_log)
# Show details of the ARIMA model
best_model
summary(best_model)
# Number of days to predict
days_forecast  <- 30
# Number of include in the plot
days_in_plot   <- 20
# Make predictions using the forecast() function
forecast_30days <- forecast(best_model, days_forecast)
# Plot
plot(forecast(best_model, days_forecast), include=days_in_plot,
xlab="Time", ylab="log(global cases)", type="o", lwd=2)
# Plot
plot(forecast(best_model, days_forecast), include=days_in_plot,
xlab="Time", ylab="log(global cases)", type="o", lwd=2)
# Plot
plot(forecast(best_model, days_forecast), include=days_in_plot,
xlab="Time", ylab="log(global cases)", type="o", lwd=2)
# Number of include in the plot
days_in_plot   <- 30
# Make predictions using the forecast() function
forecast_30days <- forecast(best_model, days_forecast)
# Plot
plot(forecast(best_model, days_forecast), include=days_in_plot,
xlab="Time", ylab="log(global cases)", type="o", lwd=2)
# Plot
plot(forecast(best_model, days_forecast),
xlab="Time", ylab="log(global cases)", type="o", lwd=2)
# Plot
plot(forecast(best_model, days_forecast), include=days_in_plot,
xlab="Time", ylab="log(global cases)", type="o", lwd=2)
# Number of include in the plot
days_in_plot   <- 5
# Make predictions using the forecast() function
forecast_30days <- forecast(best_model, days_forecast)
# Plot
plot(forecast(best_model, days_forecast), include=days_in_plot,
xlab="Time", ylab="log(global cases)", type="o", lwd=2)
#-------------------------------------------------------------------------------
# 10. Get predicted values
#-------------------------------------------------------------------------------
# Get predicted values on Nov 10, 2020
day_forward <- yday(as.Date("2020-11-10")) - yday(Date_end)
exp(forecast_30days$mean[day_forward])
exp(forecast_30days$lower[day_forward,1])
exp(forecast_30days$upper[day_forward,1])
# Get predicted values on Nov 10, 2020
day_forward <- yday(as.Date("2020-11-10")) - yday(Date_end)
exp(forecast_30days$mean[day_forward])
exp(forecast_30days$lower[day_forward,1])
exp(forecast_30days$upper[day_forward,1])
# Get predicted values on Nov 30, 2020
day_forward <- yday(as.Date("2020-11-30")) - yday(Date_end)
exp(forecast_30days$mean[day_forward])
exp(forecast_30days$lower[day_forward,1])
exp(forecast_30days$upper[day_forward,1])
# Nov 10
(500063.9 - 502287)/502287 * 100
# Nov 30
(613331.2 - 516616)/516616 * 100
(500063.9 - 502287)/502287 * 100
setwd("C://ese335")
rmarkdown::render_site()
rmarkdown::render_site()
rmarkdown::render_site()
rmarkdown::render_site()
setwd("C://ese335")
rmarkdown::render_site()
rmarkdown::render_site()
rmarkdown::render_site()
rmarkdown::render_site()
