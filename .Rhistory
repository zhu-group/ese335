# OK, let's take a quick look at the data
hist(sample1)
# Drawing a sample (n=100) from a normal distribution
sample1 <- rnorm(n=100, mean=0, sd=1)
# OK, let's take a quick look at the data
hist(sample1)
# Drawing a sample (n=100) from a normal distribution
sample1 <- rnorm(n=100, mean=0, sd=1)
# OK, let's take a quick look at the data
hist(sample1)
# Drawing a sample (n=100) from a normal distribution
sample1 <- rnorm(n=100, mean=0, sd=1)
# OK, let's take a quick look at the data
hist(sample1)
# Drawing a sample (n=100) from a normal distribution
sample1 <- rnorm(n=100, mean=0, sd=1)
# OK, let's take a quick look at the data
hist(sample1)
# Drawing a sample (n=100) from a normal distribution
sample1 <- rnorm(n=100, mean=0, sd=1)
# OK, let's take a quick look at the data
hist(sample1)
# Drawing a sample (n=100) from a normal distribution
sample1 <- rnorm(n=100, mean=0, sd=1)
# OK, let's take a quick look at the data
hist(sample1)
# Drawing a sample (n=100) from a normal distribution
sample1 <- rnorm(n=100, mean=0, sd=1)
# OK, let's take a quick look at the data
hist(sample1)
mean(sample1)
sd(sample1)
library(moments)
skewness(sample1)
kurtosis(sample1)
sample2 <- exp(sample1)
hist(sample2)
skewness(sample2)
kurtosis(sample2)
sample1
skewness(sample2)
kurtosis(sample2)
sample2
hist(sample2)
hist(log(sample2))
z <- c( rnorm(10,0,1), runif(50, 1, 2) )
hist(z)
mean(z)
sd(z)
skewness(z)
#-------------------------------------------------
# Ex 2
#-------------------------------------------------
# Central Limit Theorem
Simulations <- 10000
Samle_size  <- 100
Sample_mean <- c()
# We draw samples form a unifrom distribution
sample <- runif(Samle_size, min = 0, max = 10)
# Sample for 10000 times
for(i in 1:Simulations){
# Sample from a distribution
sample <- runif(Samle_size, min = 0, max = 10)
# Compute the sample average
average <- mean(sample)
# Store the sample average
Sample_mean <- c(Sample_mean, average)
}
# Plot hist of sample averages
hist(Sample_mean)
# Check mean, sd, and skewness
mean(Sample_mean)
sd(Sample_mean)
skewness(Sample_mean)
# Central Limit Theorem
Simulations <- 10000
Samle_size  <- 100
Sample_mean <- c()
# We draw samples form a unifrom distribution
#sample <- runif(Samle_size, min = 0, max = 10)
# Sample for 10000 times
for(i in 1:Simulations){
# Sample from a distribution
sample <- runif(Samle_size, min = 0, max = 10)
# Compute the sample average
average <- mean(sample)
# Store the sample average
Sample_mean <- c(Sample_mean, average)
}
# Plot hist of sample averages
hist(Sample_mean)
hist(z)
mean(z)
sd(z)
skewness(z)
#-------------------------------------------------
# Ex 2
#-------------------------------------------------
# Central Limit Theorem
Simulations <- 10000
Samle_size  <- 100
Sample_mean <- c()
# Sample for 10000 times
for(i in 1:Simulations){
# Sample from a distribution
Sample <- runif(Samle_size, min = 0, max = 10)
# Compute the sample average
average <- mean(Sample)
# Store the sample average
Sample_mean <- c(Sample_mean, average)
}
# Plot hist of sample averages
hist(Sample_mean)
# Central Limit Theorem
Simulations <- 10000
Samle_size  <- 20
Sample_mean <- c()
# Sample for 10000 times
for(i in 1:Simulations){
# Sample from a distribution
Sample <- runif(Samle_size, min = 0, max = 10)
# Compute the sample average
average <- mean(Sample)
# Store the sample average
Sample_mean <- c(Sample_mean, average)
}
# Plot hist of sample averages
hist(Sample_mean)
# Check mean, sd, and skewness
mean(Sample_mean)
sd(Sample_mean)
skewness(Sample_mean)
# Central Limit Theorem
Simulations <- 10000
Samle_size  <- 20
Sample_mean <- c()
# Sample for 10000 times
for(i in 1:Simulations){
# Sample from a distribution
Sample <- exp(runif(Samle_size, min = 0, max = 10))
# Compute the sample average
average <- mean(Sample)
# Store the sample average
Sample_mean <- c(Sample_mean, average)
}
# Plot hist of sample averages
hist(Sample_mean)
# Central Limit Theorem
Simulations <- 10000
Samle_size  <- 100
Sample_mean <- c()
# Sample for 10000 times
for(i in 1:Simulations){
# Sample from a distribution
Sample <- exp(runif(Samle_size, min = 0, max = 10))
# Compute the sample average
average <- mean(Sample)
# Store the sample average
Sample_mean <- c(Sample_mean, average)
}
# Plot hist of sample averages
hist(Sample_mean)
# Check mean, sd, and skewness
mean(Sample_mean)
sd(Sample_mean)
skewness(Sample_mean)
z <- c( rnorm(10,0,1), runif(50, 1, 2) )
hist(z)
# Set up
Simulations <- 10000
Samle_size  <- 100
Sample_mean <- c()
# Sample for 10000 times
for(i in 1:Simulations){
# Sample from a distribution
Sample <- runif(Samle_size, min = 0, max = 10)
# Compute the sample average
average <- mean(Sample)
# Store the sample average
Sample_mean <- c(Sample_mean, average)
}
# Plot hist of sample averages
hist(Sample_mean)
# Check mean, sd, and skewness
mean(Sample_mean)
sd(Sample_mean)
skewness(Sample_mean)
# Need `gtools` package
library(gtools)
# Obs from group A
Obs_A   <- c(1.0, 2.0, 3.0)
# Obs from group B
Obs_B   <- c(2.0, 3.0, 4.0)
# Compute the difference, d
Obs_difference <- mean(Obs_A) - mean(Obs_B)
print(Obs_difference)
Obs_all     <- c(Obs_A, Obs_B)
Obs_all
Obs_all     <- c(Obs_A, Obs_B)
Groupings_A <- combinations(length(Obs_all), length(Obs_A), Obs_all, F)
Groupings_A
?combinations
# Make an empty list
difference <- c()
# Loop all possible grouping methods for A
for(i in 1:dim(Groupings_A)[1]){
# Mean of group A
mean_A <- mean(Groupings_A[i,])
# Mean of group B
mean_B <- (sum(Obs_all)-sum(Groupings_A[i,]))/length(Obs_B)
# Store difference
difference <- c(difference, mean_A - mean_B)
}
# Show all possible differences
print(difference)
# Plot all possible differences
hist(difference)
# Add a vertical line
abline(v=Obs_difference, col="red", lwd=5, lty=2)
# Plot all possible differences
hist(difference)
# Add a vertical line
abline(v=Obs_difference, col="red", lwd=5, lty=2)
difference
# Plot all possible differences
hist(difference)
abline(v=Obs_difference, col="red", lwd=5, lty=2)
which(difference <= Obs_difference)
# Compute p-value
length( which(difference <= Obs_difference) )/length(difference)
# Need `gtools` package
library(gtools)
# Obs from group A
Obs_A   <- c(1.0, 2.0, 3.0)
# Obs from group B
Obs_B   <- c(20.0, 30.0, 40.0)
# Compute the difference, d
Obs_difference <- mean(Obs_A) - mean(Obs_B)
print(Obs_difference)
# Given H0 is true, we assume that A and B are from the same population
# So the total possible groupings for A is C(6,3)
Obs_all     <- c(Obs_A, Obs_B)
Groupings_A <- combinations(length(Obs_all), length(Obs_A), Obs_all, F)
# Show all possible groupings of A
print(Groupings_A)
# Make an empty list
difference <- c()
# Loop all possible grouping methods for A
for(i in 1:dim(Groupings_A)[1]){
# Mean of group A
mean_A <- mean(Groupings_A[i,])
# Mean of group B
mean_B <- (sum(Obs_all)-sum(Groupings_A[i,]))/length(Obs_B)
# Store difference
difference <- c(difference, mean_A - mean_B)
}
# Show all possible differences
print(difference)
# Plot all possible differences
hist(difference)
# Add a vertical line
abline(v=Obs_difference, col="red", lwd=5, lty=2)
length( which(difference <= Obs_difference) )/length(difference)
# Obs from group A
Obs_A   <- c(2.0, 3.0, 4.0, 5.0, 6.0)
# Obs from group B
Obs_B   <- c(1.0, 2.0, 3.0, 4.0)
# Compute the difference, d
Obs_difference <- mean(Obs_A) - mean(Obs_B)
print(Obs_difference)
# Given H0 is true, we assume that A and B are from the same population
# So the total possible groupings for A is C(6,3)
Obs_all     <- c(Obs_A, Obs_B)
Groupings_A <- combinations(length(Obs_all), length(Obs_A), Obs_all, F)
# Show all possible groupings of A
print(Groupings_A)
# Make an empty list
difference <- c()
# Loop all possible grouping methods for A
for(i in 1:dim(Groupings_A)[1]){
# Mean of group A
mean_A <- mean(Groupings_A[i,])
# Mean of group B
mean_B <- (sum(Obs_all)-sum(Groupings_A[i,]))/length(Obs_B)
# Store difference
difference <- c(difference, mean_A - mean_B)
}
# Show all possible differences
print(difference)
# Plot all possible differences
hist(difference)
# Add a vertical line
abline(v=Obs_difference, col="red", lwd=5, lty=2)
# Compute p-value
length( which(difference >= Obs_difference) )/length(difference)
# Obs from group A
Obs_A   <- c(2.0, 3.0, 4.0, 5.0, 6.0)
# Obs from group B
Obs_B   <- c(1.0, 2.0, 3.0, 4.0)
# Compute the difference, d
Obs_difference <- mean(Obs_A) - mean(Obs_B)
print(Obs_difference)
# Given H0 is true, we assume that A and B are from the same population
# So the total possible groupings for A is C(6,3)
Obs_all     <- c(Obs_A, Obs_B)
Groupings_A <- combinations(length(Obs_all), length(Obs_A), Obs_all, F)
# Show all possible groupings of A
print(Groupings_A)
# Make an empty list
difference <- c()
# Loop all possible grouping methods for A
for(i in 1:dim(Groupings_A)[1]){
# Mean of group A
mean_A <- mean(Groupings_A[i,])
# Mean of group B
mean_B <- (sum(Obs_all)-sum(Groupings_A[i,]))/length(Obs_B)
# Store difference
difference <- c(difference, mean_A - mean_B)
}
# Show all possible differences
print(difference)
# Plot all possible differences
hist(difference)
Obs_difference
# Add a vertical line
abline(v=Obs_difference, col="red", lwd=5, lty=2)
abline(v=Obs_difference*(-1), col="red", lwd=5, lty=2)
# to test is:
# whether it HAS IMPACT on student’s scores
length( which(difference >= Obs_difference) )/length(difference) +
length( which(difference <= Obs_difference*(-1)) )/length(difference)
# Add a vertical line
abline(v=Obs_difference, col="red", lwd=5, lty=2)
abline(v=Obs_difference*(-1), col="red", lwd=5, lty=2)
# Compute p-value
# This is one-sided p-value, since the question we want
# to test is:
# whether it is useful in INCREASING student’s scores
length( which(difference >= Obs_difference) )/length(difference)
#---------------------------------------
# Ex. 1
#---------------------------------------
# Obs from group A
Obs_A   <- c(2.0, 3.0, 4.0, 5.0, 6.0)
# Obs from group B
Obs_B   <- c(1.0, 2.0, 3.0, 4.0)
# Compute the difference, d
Obs_difference <- mean(Obs_A) - mean(Obs_B)
print(Obs_difference)
# Given H0 is true, we assume that A and B are from the same population
# So the total possible groupings for A is C(6,3)
Obs_all     <- c(Obs_A, Obs_B)
Groupings_A <- combinations(length(Obs_all), length(Obs_A), Obs_all, F)
# Show all possible groupings of A
print(Groupings_A)
# Make an empty list
difference <- c()
# Loop all possible grouping methods for A
for(i in 1:dim(Groupings_A)[1]){
# Mean of group A
mean_A <- mean(Groupings_A[i,])
# Mean of group B
mean_B <- (sum(Obs_all)-sum(Groupings_A[i,]))/length(Obs_B)
# Store difference
difference <- c(difference, mean_A - mean_B)
}
# Show all possible differences
print(difference)
# Plot all possible differences
hist(difference)
# Add a vertical line
abline(v=Obs_difference, col="red", lwd=5, lty=2)
# Compute p-value
# This is one-sided p-value, since the question we want
# to test is:
# whether it is useful in INCREASING student’s scores
length( which(difference >= Obs_difference) )/length(difference)
#---------------------------------------
# Ex. 2
#---------------------------------------
# Obs from group A
Obs_A   <- c(2.0, 3.0, 4.0, 5.0, 6.0)
# Obs from group B
Obs_B   <- c(1.0, 2.0, 3.0, 4.0)
# Compute the difference, d
Obs_difference <- mean(Obs_A) - mean(Obs_B)
print(Obs_difference)
# Given H0 is true, we assume that A and B are from the same population
# So the total possible groupings for A is C(6,3)
Obs_all     <- c(Obs_A, Obs_B)
Groupings_A <- combinations(length(Obs_all), length(Obs_A), Obs_all, F)
# Show all possible groupings of A
print(Groupings_A)
# Make an empty list
difference <- c()
# Loop all possible grouping methods for A
for(i in 1:dim(Groupings_A)[1]){
# Mean of group A
mean_A <- mean(Groupings_A[i,])
# Mean of group B
mean_B <- (sum(Obs_all)-sum(Groupings_A[i,]))/length(Obs_B)
# Store difference
difference <- c(difference, mean_A - mean_B)
}
# Show all possible differences
print(difference)
# Plot all possible differences
hist(difference)
# Add a vertical line
abline(v=Obs_difference, col="red", lwd=5, lty=2)
abline(v=Obs_difference*(-1), col="red", lwd=5, lty=2)
# Compute p-value
# This is two-sided p-value, since the question we want
# to test is:
# whether it HAS IMPACT on student’s scores
length( which(difference >= Obs_difference) )/length(difference) +
length( which(difference <= Obs_difference*(-1)) )/length(difference)
library(ggpubr)
library(nortest)
library(geoR)
# Make a vector from -5 to 5, with a step of 0.01
x        <- seq(-5.0, 5.0, by=0.01)
# Compute the density for each element in x
density1 <- dnorm(x, 0, 1)
# Plot Density
plot(x, density1, col="black", xlab="", ylab="Density",
type="l", lwd=3, cex=2,
xlim=c(-5.0, 5.0),
main="PDF of normal distributions")
# Compute and plot the density from another normal
density2  <- dnorm(x, 1, 1)
lines(x, density2, col="blue", xlab="", ylab="Density",
type="l", lwd=3, cex=2)
# Compute and plot the density from another normal
density3  <- dnorm(x, 0, 3)
lines(x, density3, col="red", xlab="", ylab="Density",
type="l", lwd=3, cex=2)
# Add legends
text(-4, 0.35, "N(0,1)", col="black",   cex=2)
text(-4, 0.30, "N(1,1)", col="blue",  cex=2)
text(-4, 0.25, "N(0,3)", col="red", cex=2)
# Sample 1 is from a normal distribution
sample1  <- rnorm(200, 0, 1)
# Sample 2 is from a uniform distribution
sample2  <- runif(200, 0, 1)
# Plot density function of sample 1
ggdensity(sample1, main = "Density plot of sample 1",
xlab = "x", color ="blue", lwd=1.5)
# Plot density function of sample 2
ggdensity(sample2, main = "Density plot of sample 2",
xlab = "x", color ="red", lwd=1.5)
# QQ plot of sample1
ggqqplot(sample1)
# QQ plot of sample2
ggqqplot(sample2)
# Sample 3 is from a normal distribution
sample3  <- sample1[1:25]
# Sample 4 is from a uniform distribution
sample4  <- sample2[1:25]
# Shapiro-Wilk test of sample 3
shapiro.test(sample3)
# Shapiro-Wilk test of sample 4
shapiro.test(sample4)
# Lilliefors test of sample 1
lillie.test(sample1)
# Lilliefors test of sample 2
lillie.test(sample2)
# Sample 5 is from a log-normal distribution
sample5  <- exp(rnorm(200, 0, 1))
# Plot density function of sample 1
ggdensity(sample5, main = "Density plot of sample 5",
xlab = "x", color ="blue", lwd=1.5)
# Sample 5 is from a log-normal distribution
sample5  <- exp(rnorm(200, 0, 1))
sample6  <- log(sample5)
# Plot density function of sample 1
ggdensity(sample6, main = "Density plot of sample 6",
xlab = "x", color ="blue", lwd=1.5)
# Sample 5 is from a log-normal distribution
sample5  <- exp(rnorm(200, 0, 1))
# Plot density function of sample 1
ggdensity(sample5, main = "Density plot of sample 5",
xlab = "x", color ="blue", lwd=1.5)
# Sample 5 is from a log-normal distribution
sample6  <- log(sample5)
# Plot density function of sample 1
ggdensity(sample6, main = "Density plot of sample 6",
xlab = "x", color ="blue", lwd=1.5)
# Generate a sample
y <- exp(runif(30,0,1)+runif(30,0,1)+runif(30,0,1))
# Plot the density
ggdensity(y,main = "Density plot of sample", xlab = "x")
# Plot Q-Q plot
ggqqplot(y)
# Lilliefors test
lillie.test(y)
# BOX-COX transformation, get lambda
lambda <- boxcoxfit(y)$lambda
# BOX-COX transformation
y_new <- (y^lambda-1)/lambda
# Plot the density
ggdensity(y_new,main = "Density plot of sample", xlab = "x")
# Plot Q-Q plot
ggqqplot(y_new)
# Lilliefors test
lillie.test(y_new)
setwd("C://ese335")
rmarkdown::render_site()
rmarkdown::render_site()
setwd("C://ese335")
rmarkdown::render_site()
rmarkdown::render_site()
