t_beta1        <- Beta1_hat / SE_beta1_hat
t_beta1
# Find the two-side p-value
# The pt function gives the Cumulative Distribution Function (CDF)
# of the Student's t distribution in R, which is the probability that
# the variable takes a value lower or equal to a threshold (here |t|).
P_value        <- (1-pt(abs(t_beta1), df=df_error))*2
print(P_value)
# Make confidence band
predict(reg, interval="confidence", level=0.95)
# dependent variable (Uptaken_amount) and
# independent variable (Soil_conc)
reg            <- lm( Uptaken_amount ~ Soil_conc )
# Print details of the linear model
summary(reg)
# Print the ANOVA table
anova(reg)
# Make confidence band
predict(reg, interval="confidence", level=0.95)
# Make Prediction band
predict(reg, interval="prediction", level=0.95)
# Build the model
reg             <- lm(Uptaken_amount ~ Soil_conc, data=Pesticide_data)
summary(reg)
# Build the model
reg             <- lm(Uptaken_amount ~ Soil_conc, data=Pesticide_data)
summary(reg)
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
TOC <- c(1.59, 1.79, 1.00, 0.45, 3.84, 2.30,
1.00, 3.20, 0.80, 2.20, 3.20, 2.10)
UV <- c(0.144, 0.152, 0.113, 0.025, 0.227,
0.149, 0.014, 0.158, 0.042, 0.154, 0.173,
0.095)
# Fit a simple linear regression model
reg            <- lm( UV ~ TOC )
# Print details of the linear model
summary(reg)
# Check independence of the dependent variable (Y)
runs.test(TOC)
# Scatter plot
# Do you see a linear trend
plot(UV, TOC)
# Check potential outliers
boxplot(TOC)
grubbs.test(TOCt)
grubbs.test(TOC)
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# Make confidence band
predict(reg, interval="confidence", level=0.95)
# Make Prediction band
predict(reg, interval="prediction", level=0.95)
# Make confidence band
predict(reg, 2.0, interval="confidence", level=0.95)
# Make confidence band
predict(reg, c(2.0), interval="confidence", level=0.95)
?predict
# Fit a simple linear regression model
reg            <- lm( UV ~ TOC )
# Print details of the linear model
summary(reg)
grubbs.test(UV)
# Check assumptions and normality and equal variance
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
predict(reg, new, interval="confidence", level=0.95)
TOC <- c(1.59, 1.79, 1.00, 0.45, 3.84, 2.30,
1.00, 3.20, 0.80, 2.20, 3.20, 2.10)
UV <- c(0.144, 0.152, 0.113, 0.025, 0.227,
0.149, 0.014, 0.158, 0.042, 0.154, 0.173,
0.095)
# Check independence of the dependent variable (Y)
runs.test(UV)
# Scatter plot
# Do you see a linear trend
plot(TOC, UV)
# Check potential outliers
boxplot(UV)
grubbs.test(UV)
# Fit a simple linear regression model
reg            <- lm( UV ~ TOC )
# Print details of the linear model
summary(reg)
library(ggplot2)
library(randtests)
library(outliers)
# Observations
Soil_conc      <- c(10, 50, 20, 30, 80, 60, 70, 40)
Uptaken_amount <- c(0.18, 1.05, 0.50, 0.61, 1.58, 1.10, 1.36, 0.77)
# Degrees of freedom
# Model, simple linear regressionâ€“the slope and intercept
df_regression  <- 2 - 1 # 2 parameters
# Error
df_error       <- length(Soil_conc) - 2
# Get estimator of beta1 and beta0 manually
Beta1_hat      <- cov(Soil_conc, Uptaken_amount)/sd(Soil_conc)^2
Beta0_hat      <- mean(Uptaken_amount) - Beta1_hat*mean(Soil_conc)
# Predictions based on the linear model
Prediction     <- Beta0_hat + Soil_conc*Beta1_hat
# Variance analysis
SST            <- sum( (Uptaken_amount - mean(Uptaken_amount))^2 )
SSR            <- sum( (Prediction - mean(Uptaken_amount))^2   )
SSE            <- sum( (Uptaken_amount- Prediction)^2 )
# MSE
MSE            <- SSE/df_error
# Get SE of residual
print(sqrt(MSE))
# SE of beta1_hat
SE_beta1_hat   <- sqrt(MSE/sum((Soil_conc-mean(Soil_conc))^2))
# Get t statistic
t_beta1        <- Beta1_hat / SE_beta1_hat
# Find the two-side p-value
# The pt function gives the Cumulative Distribution Function (CDF)
# of the Student's t distribution in R, which is the probability that
# the variable takes a value lower or equal to a threshold (here |t|).
P_value        <- (1-pt(abs(t_beta1), df=df_error))*2
print(P_value)
# Get F-statistic
MSR            <- SSR/df_regression
F_ratio        <- MSR/MSE
# Find the p-value
# The pf() function gives the Cumulative Distribution Function (CDF)
# of the F distribution in R, which is the probability that
# the variable takes a value lower or equal to a threshold (here F_ratio).
# Here we that 1-pf to get the probability that the
# variable takes a value higher than the threshold (F_ratio).
P_value        <- 1 - pf(F_ratio, df1=df_regression, df2=df_error)
print(P_value)
# R-squared
R2             <- SSR/SST
print(R2)
# Adjusted R-squared
n              <- length(Soil_conc)
k              <- 1 # only beta1 counts
R2_adj         <- 1 - (1-R2)*(n-1)/(n-k-1)
print(R2_adj)
# Fit a simple linear regression model between
# dependent variable (Uptaken_amount) and
# independent variable (Soil_conc)
reg            <- lm( Uptaken_amount ~ Soil_conc )
# Print details of the linear model
summary(reg)
# Print the ANOVA table
anova(reg)
# Make confidence band
predict(reg, interval="confidence", level=0.95)
# Make Prediction band
predict(reg, interval="prediction", level=0.95)
# Make data frame
Pesticide_data  <- data.frame(Soil_conc,Uptaken_amount)
# Build the model
reg             <- lm(Uptaken_amount ~ Soil_conc, data=Pesticide_data)
# Make predictions for individual responses
Pred_band       <- predict(reg, interval="prediction", level=0.95)
# Update data frame
Pesticide_data2 <- cbind(Pesticide_data, Pred_band)
# Plot
ggplot(Pesticide_data2, aes(Soil_conc, Uptaken_amount))+
geom_point() +
geom_line(aes(y=lwr), color = "red", linetype = "dashed")+
geom_line(aes(y=upr), color = "red", linetype = "dashed")+
geom_smooth(method=lm, se=TRUE)
# Check independence of the dependent variable (Y)
runs.test(Pesticide_data$Uptaken_amount)
# Scatter plot
# Do you see a linear trend
plot(Pesticide_data$Uptaken_amount, Pesticide_data$Soil_conc)
# Check potential outliers
boxplot(Pesticide_data$Uptaken_amount)
grubbs.test(Pesticide_data$Uptaken_amount)
# Build the model
reg             <- lm(Uptaken_amount ~ Soil_conc, data=Pesticide_data)
summary(reg)
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
#--------------------------
# EX. 1
TOC <- c(1.59, 1.79, 1.00, 0.45, 3.84, 2.30,
1.00, 3.20, 0.80, 2.20, 3.20, 2.10)
UV <- c(0.144, 0.152, 0.113, 0.025, 0.227,
0.149, 0.014, 0.158, 0.042, 0.154, 0.173,
0.095)
# Check independence of the dependent variable (Y)
runs.test(UV)
# Scatter plot
# Do you see a linear trend
plot(TOC, UV)
# Check potential outliers
boxplot(UV)
grubbs.test(UV)
# Fit a simple linear regression model
reg            <- lm( UV ~ TOC )
# Print details of the linear model
summary(reg)
# Check assumptions and normality and equal variance
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# Make confidence band
new <- data.frame(TOC = c(2.0))
predict(reg, new, interval="confidence", level=0.95)
# Make Prediction band
predict(reg, interval="prediction", level=0.95)
new
predict(reg, new, interval="confidence", level=0.95)
# Make Prediction band, individual response
predict(reg, new_obs, interval="prediction", level=0.95)
# Make confidence band, mean response
new_obs <- data.frame(TOC = c(2.0))
predict(reg, new_obs, interval="confidence", level=0.95)
# Make Prediction band, individual response
predict(reg, new_obs, interval="prediction", level=0.95)
library(ggplot2)
library(GGally)
library(olsrr)
# Read ozone and meteorological data
Ozone_data <- read.csv(file = "ozone_data.csv", header=T)
# Check the data
str(Ozone_data)
# Plot scatter plots
ggpairs(Ozone_data, columns=1:5)
library(ggplot2)
library(GGally)
library(olsrr)
# Read ozone and meteorological data
Ozone_data <- read.csv(file = "ozone_data.csv", header=T)
# Check the data
str(Ozone_data)
# Plot scatter plots
ggpairs(Ozone_data, columns=1:5)
#----------------------------------------------------------------------
# Fit the full model, where all independent variables are included
full_model  <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature + Pressure, data = Ozone_data)
# Test all possible subsets
output      <- ols_step_all_possible(full_model)
# Print results from all possible subsets
output
# Plot results from all possible subsets
plot(output)
#----------------------------------------------------------------------
ols_step_best_subset(full_model)
ols_step_backward_aic(full_model, details=F)
ols_step_forward_aic(full_model, details=F)
ols_step_both_aic(full_model, details=F)
ols_step_backward_aic(full_model, details=T)
#----------------------------------------------------------------------
# Best model
reg <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature , data = Ozone_data)
summary(reg)
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# Make confidence band
predict(reg, interval="confidence", level=0.95)
# Make prediction band
predict(reg, interval="prediction", level=0.95)
# 1.1 Load data
library(MASS)
data(cpus)
# 1.2 Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
cpus_validation
# 1.3 Plot scatter plots
ggpairs(cpus_training, columns=2:9)
# 1.4 Full model
full_model <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmin + chmax, data = cpus_training)
summary(full_model)
# Variable selection
# 1.5 Use the all possible subsets approach to find the best model
ols_step_all_possible(full_model)
# 1.6 Use the all best subsets approach to find the best model
ols_step_best_subset(full_model)
# 1.7 Use backward elimination approach to find the best model.
ols_step_backward_aic(full_model)
# 1.8 Use forward selection approach to find the best model.
ols_step_forward_aic(full_model)
# 1.9 Use stepwise regression approach to find the best model.
ols_step_both_aic(full_model)
# 1.10 Best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
summary(reg)
# Check assumptions
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# 1.11 Predicted individual CPU performance of the validation subset
pred <- predict(reg, cpus_validation,interval="prediction", level=0.95)
# Compare
plot(pred[,1],cpus_validation$perf, xlab="Predicted CPU performance",
ylab="Actual CPU performance")
# Get Pearson correlation coefficient and relative mean bias
cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias <- mean(pred[,1]) - mean(cpus_validation$perf)
mean_bias/mean(cpus_validation$perf)*100
# 1.12 Simulations
N_simulations     <- 50
correlation_all   <- c()
relative_bias_all <- c()
# Loop each simulation
for(i in 1:N_simulations){
# Create new subsets each time
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Using the same best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
# Mean Pearson correlation coefficient
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
mean(correlation_all) # ~ 0.90, not bad
# 1.12 Simulations
N_simulations     <- 50
correlation_all   <- c()
relative_bias_all <- c()
# Loop each simulation
for(i in 1:N_simulations){
# Create new subsets each time
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Using the same best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
# Mean Pearson correlation coefficient
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
mean(correlation_all) # ~ 0.90, not bad
# 1.12 Simulations
N_simulations     <- 50
correlation_all   <- c()
relative_bias_all <- c()
# Loop each simulation
for(i in 1:N_simulations){
# Create new subsets each time
# Partitioning training (85%) and validation (15%) subsets
fraction        <- 0.85
sample_index    <- sample(nrow(cpus),nrow(cpus)*fraction)
cpus_training   <- cpus[sample_index,]
cpus_validation <- cpus[-sample_index,]
# Using the same best model
reg <- lm(perf  ~ syct   + mmin   + mmax  + cach + chmax, data = cpus_training)
pred <- predict(reg, cpus_validation, interval="prediction", level=0.95)
# Get Pearson correlation coefficient and relative mean bias
cor_temp <- cor(pred[,1],cpus_validation$perf)
# Get relative mean bias
mean_bias          <- mean(pred[,1]) - mean(cpus_validation$perf)
relative_bias_temp <- mean_bias/mean(cpus_validation$perf)*100
# Store
correlation_all   <- c(correlation_all, cor_temp)
relative_bias_all <- c(relative_bias_all, relative_bias_temp)
}
# Mean Pearson correlation coefficient
hist(correlation_all)
abline(v=mean(correlation_all), col="red")
mean(correlation_all) # ~ 0.90, not bad
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all) # 1-2%, very good!
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all) # 1-2%, very good!
# Mean relative mean bias
hist(relative_bias_all)
abline(v=mean(relative_bias_all), col="blue")
mean(relative_bias_all) # 1-2%, very good!
library(ggplot2)
library(GGally)
library(olsrr)
# Read ozone and meteorological data
Ozone_data <- read.csv(file = "ozone_data.csv", header=T)
# Check the data
str(Ozone_data)
# Plot scatter plots
ggpairs(Ozone_data, columns=1:5)
#----------------------------------------------------------------------
# Fit the full model, where all independent variables are included
full_model  <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature + Pressure, data = Ozone_data)
full_model
summary(full_model)
output      <- ols_step_all_possible(full_model)
# Print results from all possible subsets
output
output      <- ols_step_all_possible(full_model)
# Print results from all possible subsets
output
# Plot results from all possible subsets
plot(output)
# Plot results from all possible subsets
plot(output)
#----------------------------------------------------------------------
ols_step_best_subset(full_model)
ols_step_backward_aic(full_model, details=T)
ols_step_forward_aic(full_model, details=F)
ols_step_forward_aic(full_model, details=T)
ols_step_both_aic(full_model, details=F)
reg <- lm(Ozone ~ Solar.Rad + Wind.Speed + Temperature , data = Ozone_data)
summary(reg)
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
par(mfrow = c(2, 2))
plot(reg)
par(mfrow = c(1, 1))
# Make confidence band
predict(reg, interval="confidence", level=0.95)
# Make prediction band
predict(reg, interval="prediction", level=0.95)
q1 <- c(100, 50, 100, 100, 90, 98, 100, 100)
mean(q1)
sd(q1)
q2 <- c(85, 80, 100, 100, 75, 90, 100, 100)
mean(q2)
sd(q2)
q3 <- c(100, 90, 50, 80, 80, 80, 100, 100)
mean(q3)
sd(q3)
q4 <- c(100, 20, 20, 70, 70, 80, 90, 80)
mean(q4)
sd(q4)
# Population per km2
Pop <- c(797,  3652,   384,   876,  1156,
5282,  3602,  4305,  6451, 939,
2725,   296,  1187,  4819,  7856,
1074,  1444,  2620,   417,  3232)
# PM exceeding
PM  <- c( 0,     1,    0,   0,   0,
1,     0,    1,   1,   1,
1,     0,    0,   0,   1,
0,     0,    1,   0,   1)
# Make a data frame
PM_data <- data.frame(Pop, PM)
str(PM_data)
# Fit the regression model
logistic <- glm( PM ~ Pop, data = PM_data, family = binomial)
# Print model detail
summary(logistic)
library(pscl)
library(InformationValue)
# Population per km2
Pop <- c(797,  3652,   384,   876,  1156,
5282,  3602,  4305,  6451, 939,
2725,   296,  1187,  4819,  7856,
1074,  1444,  2620,   417,  3232)
# PM exceeding
PM  <- c( 0,     1,    0,   0,   0,
1,     0,    1,   1,   1,
1,     0,    0,   0,   1,
0,     0,    1,   0,   1)
# Make a data frame
PM_data <- data.frame(Pop, PM)
str(PM_data)
# Fit the regression model
logistic <- glm( PM ~ Pop, data = PM_data, family = binomial)
# Print model detail
summary(logistic)
pR2(logistic)["McFadden"]
# Find optimal cutoff probability to use to maximize accuracy
predicted_value <- predict(logistic, PM_data, type="response")
optimalCutoff(PM_data$PM, predicted_value)[1]
# Define new cities where we want to make predictions
new_cities <- data.frame(Pop = c(1000, 5000))
#predict probability of defaulting
predict(logistic, new_cities, type="response")
setwd("C://ese335")
rmarkdown::render_site()
rmarkdown::render_site()
